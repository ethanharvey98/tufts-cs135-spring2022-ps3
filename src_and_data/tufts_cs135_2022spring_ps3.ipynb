{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRn8qaWAUxu"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "This file contains code that helps you get started on the programming assignment. You will need to complete the function `sample_images()`, `sparse_auto_encoder()`, and `compute_numerical_gradient()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smB4STFGATeW"
      },
      "source": [
        "**STEP 0:** Here we provide the relevant parameters values that will allow your sparse autoencoder to get good filters; you do not need to change the parameters below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1_f8AQzFZek"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import scipy\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hglZn99AIBL"
      },
      "outputs": [],
      "source": [
        "patch_size = 8\n",
        "num_patches = 10000\n",
        "visible_size = 8*8;     # number of input units \n",
        "hidden_size = 25;       # number of hidden units \n",
        "sparsity_param = 0.01;  # desired average activation of the hidden units.\n",
        "                        # (This was denoted by the Greek alphabet rho, which \n",
        "                        # looks like a lower-case \"p\", in the lecture notes). \n",
        "decay_lambda = 0.0001;  # weight decay parameter (lambda)\n",
        "beta = 3;               # weight of sparsity penalty term "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBwj0MA1BmFz"
      },
      "source": [
        "**STEP 1:** Implement `sample_images()`\n",
        "\n",
        "After implementing `sample_images()`, the `display_network()` function should display a random sample of 100 patches from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48IeOvoobJOR"
      },
      "outputs": [],
      "source": [
        "# This function visualizes filters in matrix A. Each row of A is a\n",
        "# filter. We will reshape each row into a square image and visualizes\n",
        "# on each cell of the visualization panel.\n",
        "# All other parameters are optional, usually you do not need to worry\n",
        "# about it.\n",
        "# opt_normalize: whether we need to normalize the filter so that all of\n",
        "# them can have similar contrast. Default value is true.\n",
        "# opt_graycolor: whether we use gray as the heat map. Default is true.\n",
        "# cols: how many rows are there in the display. Default value is the\n",
        "# squareroot of the number of rows in A.\n",
        "def display_network(data, cols=-1, opt_normalize=True, opt_graycolor=True, save_figure_path=None):\n",
        "\n",
        "    # rescale\n",
        "    data -= np.mean(data)\n",
        "\n",
        "    # compute rows, cols\n",
        "    num, area = data.shape\n",
        "    sz = int(math.sqrt(area))\n",
        "    buf = 1\n",
        "    if cols < 0:\n",
        "        if math.floor(math.sqrt(num)) ** 2 != num:\n",
        "            n = math.ceil(math.sqrt(num))\n",
        "            while num % n != 0 and n < 1.2 * math.sqrt(num):\n",
        "                n += 1\n",
        "                m = math.ceil(num / n)\n",
        "        else:\n",
        "            n = math.sqrt(num)\n",
        "            m = n\n",
        "    else:\n",
        "        n = cols\n",
        "        m = math.ceil(num / n)\n",
        "    n = int(n)\n",
        "    m = int(m)\n",
        "\n",
        "    array = -np.ones((buf + m * (sz + buf), buf + n * (sz + buf)))\n",
        "\n",
        "    if not opt_graycolor:\n",
        "        array *= 0.1\n",
        "\n",
        "    k = 0\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if k >= num:\n",
        "                continue\n",
        "            if opt_normalize:\n",
        "                clim = np.amax(np.absolute(data[k, :]))\n",
        "            else:\n",
        "                clim = np.amax(np.absolute(data))\n",
        "            array[buf + i * (sz + buf):buf + i * (sz + buf) + sz,\n",
        "            buf + j * (sz + buf):buf + j * (sz + buf) + sz] = data[k, :].reshape([sz, sz]) / clim\n",
        "            k += 1\n",
        "\n",
        "    # simulate imagesc\n",
        "    ax = plt.figure().gca()\n",
        "    pix_width = 5\n",
        "    h, w = array.shape\n",
        "    exts = (0, pix_width * w, 0, pix_width * h)\n",
        "    if opt_graycolor:\n",
        "        ax.imshow(array, interpolation='nearest', extent=exts, cmap=cm.gray)\n",
        "    else:\n",
        "        ax.imshow(array, interpolation='nearest', extent=exts)\n",
        "\n",
        "    plt.axis('off')\n",
        "\n",
        "    if save_figure_path:\n",
        "        plt.savefig(save_figure_path)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjpM5nAvUjp3"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(hiddenSize, visibleSize):\n",
        "    # Initialize parameters randomly based on layer sizes.\n",
        "    r = math.sqrt(6) / math.sqrt(hidden_size + visible_size + 1)  # we'll choose weights uniformly from the interval [-r, r]\n",
        "    w1 = np.random.rand(visible_size, hidden_size) * 2 * r - r\n",
        "    w2 = np.random.rand(hidden_size, visible_size) * 2 * r - r\n",
        "\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    b2 = np.zeros((1, visible_size))\n",
        "\n",
        "    # Convert weights and bias gradients to the vector form.\n",
        "    # This step will \"unroll\" (flatten and concatenate together) all\n",
        "    # your parameters into a vector, which can then be used with minFunc.\n",
        "    theta = np.concatenate((w1.flatten(), w2.flatten(), b1.flatten(), b2.flatten()))\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-yLpdTPIft1"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "def normalize_data(patches):\n",
        "\n",
        "    return patches\n",
        "\n",
        "def sample_images(patch_size, num_patches):\n",
        "    \n",
        "    images = loadmat('IMAGES.mat')['IMAGES']  # load images from disk\n",
        "\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e-sIeVCIc_4"
      },
      "outputs": [],
      "source": [
        "patches = sample_images(patch_size, num_patches)\n",
        "display_network(patches[np.random.randint(patches.shape[0], size=100), :])\n",
        "\n",
        "# Obtain random parameters theta\n",
        "theta = initialize_parameters(hidden_size, visible_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWthF3_ZSBuz"
      },
      "source": [
        "**STEP 2:** Implement sparseAutoencoderCost\n",
        "\n",
        "You can implement all of the components (squared error cost, weight decay term, sparsity penalty) in the cost function at once, but it may be easier to do it step-by-step and run gradient checking (see STEP 3) after each step.  We suggest implementing the `sparse_autoencoder_cost()` function using the following steps:\n",
        "\n",
        "\n",
        "*   Implement forward propagation in your neural network, and implement the squared error term of the cost function.  Implement backpropagation to compute the derivatives.   Then (using lambda=beta=0), run Gradient Checking to verify that the calculations corresponding to the squared error cost term are correct.\n",
        "\n",
        "*   Add in the weight decay term (in both the cost function and the derivative calculations), then re-run Gradient Checking to verify correctness. \n",
        "\n",
        "*   Add in the sparsity penalty term, then re-run Gradient Checking to verify correctness. Feel free to change the training settings when debugging your code.  (For example, reducing the training set size or number of hidden units may make your code run faster; and setting beta and/or lambda to zero may be helpful for debugging.)  However, in your final submission of the visualized weights, please use parameters we gave in Step 0 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot_M7XxvYHnK"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# visible_size: the number of input units (probably 64)\n",
        "# hidden_size: the number of hidden units (probably 25)\n",
        "# decay_lambda: weight decay parameter\n",
        "# sparsity_param: The desired average activation for the hidden units (denoted in the lecture\n",
        "# notes by the greek alphabet rho, which looks like a lower-case \"p\").\n",
        "# beta: weight of sparsity penalty term\n",
        "# data: Our 10000x64 matrix containing the training data.  So, data(i,:) is the i-th training example.\n",
        "def sparse_autoencoder_cost(theta, visible_size, hidden_size, decay_lambda, sparsity_param, beta, data):\n",
        "\n",
        "    return cost, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYyvXW-kXqAn"
      },
      "outputs": [],
      "source": [
        "cost, grad = sparse_autoencoder_cost(theta, visible_size, hidden_size, decay_lambda, sparsity_param, beta, patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0memAWRGXwdJ"
      },
      "source": [
        "**Step 3:** Gradient Checking\n",
        "\n",
        "Hint: If you are debugging your code, performing gradient checking on smaller models and smaller training sets (e.g., using only 10 training examples and 1-2 hidden units) may speed things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXT2xQg7ULS9"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# theta: a vector of parameters\n",
        "# func: a function that outputs a real-number. Calling y = J(theta) will return the\n",
        "# function value at theta.\n",
        "def compute_numerical_gradient(func, theta, *args):\n",
        "\n",
        "    # Initialize numgrad (no need to initialize to zero, empty_like is a good fit here)\n",
        "    numgrad = np.empty_like(theta)\n",
        "\n",
        "    # Instructions: \n",
        "    # Implement numerical gradient checking, and return the result in numgrad.  \n",
        "    # (See Section 2.3 of the lecture notes.)\n",
        "    # You should write code so that numgrad(i) is (the numerical approximation to) the \n",
        "    # partial derivative of func with respect to the i-th input argument, evaluated at theta.\n",
        "    # I.e., numgrad(i) should be the (approximately) the partial derivative of func with\n",
        "    # respect to theta(i).\n",
        "    #\n",
        "    # Hint: You will probably want to compute the elements of numgrad one at a time.\n",
        "\n",
        "    return numgrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfsCip6e0slS"
      },
      "outputs": [],
      "source": [
        "# this function accepts a 2D vector as input. \n",
        "# Its outputs are:\n",
        "# value: h(x1, x2) = x1^2 + 3*x1*x2\n",
        "# grad: A 2x1 vector that gives the partial derivatives of h with respect to x1 and x2 \n",
        "# Note that when we pass @simpleQuadraticFunction(x) to computeNumericalGradients, we're assuming\n",
        "# that computeNumericalGradients will use only the first returned value of this function.\n",
        "def simple_quadratic_function(x):\n",
        "    value = pow(x[0], 2) + 3*x[0]*x[1]\n",
        "    grad = np.zeros(2)\n",
        "    grad[0]  = 2*x[0] + 3*x[1]\n",
        "    grad[1]  = 3*x[0]\n",
        "    return value, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkPGPRLoyZft"
      },
      "outputs": [],
      "source": [
        "# This code can be used to check your numerical gradient implementation \n",
        "# in computeNumericalGradient.m\n",
        "# It analytically evaluates the gradient of a very simple function called\n",
        "# simpleQuadraticFunction (see below) and compares the result with your numerical\n",
        "# solution. Your numerical gradient implementation is incorrect if\n",
        "# your numerical solution deviates too much from the analytical solution.\n",
        "def check_numerical_gradient():\n",
        "    # Evaluate the function and gradient at x = [4; 10]; (Here, x is a 2d vector.)\n",
        "    x = [4, 10]\n",
        "    value, grad = simple_quadratic_function(x)\n",
        "\n",
        "    # Use your code to numerically compute the gradient of simpleQuadraticFunction at x.\n",
        "    # (The notation \"@simpleQuadraticFunction\" denotes a pointer to a function.)\n",
        "    numgrad = compute_numerical_gradient(simple_quadratic_function, x);\n",
        "\n",
        "    # Visually examine the two gradient computations.  The two columns\n",
        "    # you get should be very similar. \n",
        "    disp = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(numgrad, grad))\n",
        "    print(disp)\n",
        "    print(\"The above two columns you get should be very similar.\\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n\");\n",
        "\n",
        "    # Evaluate the norm of the difference between two solutions.  \n",
        "    # If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
        "    # in computeNumericalGradient.m, then diff below should be 2.1452e-12 \n",
        "    diff = np.linalg.norm(numgrad-grad)/np.linalg.norm(numgrad+grad);\n",
        "    print(diff); \n",
        "    print(\"Norm of the difference between numerical and analytical gradient (should be < 1e-9)\\n\\n\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So3WH5vhyeog"
      },
      "outputs": [],
      "source": [
        "# First, lets make sure your numerical gradient computation is correct for a \n",
        "# simple function.  After you have implemented computeNumericalGradient.m, \n",
        "# run the following:\n",
        "check_numerical_gradient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF-rv9RLyuMA"
      },
      "outputs": [],
      "source": [
        "# Now we can use it to check your cost function and derivative calculations\n",
        "# for the sparse autoencoder.  \n",
        "numgrad = compute_numerical_gradient(sparse_autoencoder_cost, theta, visible_size, hidden_size, decay_lambda, sparsity_param, beta, patches)\n",
        "\n",
        "# Use this to visually compare the gradients side by side\n",
        "disp = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(numgrad, grad))\n",
        "print(disp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AweZOO7Cy8Yi"
      },
      "outputs": [],
      "source": [
        "# Compare numerically computed gradients with the ones obtained from backpropagation\n",
        "diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad);\n",
        "print(diff);  # Should be small. In our implementation, these values are\n",
        "              # usually less than 1e-9.\n",
        "              # When you got this working, Congratulations!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iUF39lCzHTf"
      },
      "source": [
        "**Step 4:** After verifying that your implementation of `sparse_autoencoder_cost()` is correct, You can start training your sparse autoencoder with scipy's minimize function (L-BFGS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wmc2s6HzOP9"
      },
      "outputs": [],
      "source": [
        "# Randomly initialize the parameters\n",
        "theta = initialize_parameters(hidden_size, visible_size);\n",
        "\n",
        "# Here, we use L-BFGS to optimize our cost\n",
        "# function. Generally, for minimize to work, you\n",
        "# need a function pointer with two outputs: the\n",
        "# function value and the gradient. In our problem,\n",
        "# sparseAutoencoderCost.m satisfies this.\n",
        "\n",
        "# Use scipy's minimize to minimize the function\n",
        "res = scipy.optimize.minimize(\n",
        "    fun=sparse_autoencoder_cost, \n",
        "    x0=theta, \n",
        "    args=(visible_size, hidden_size, decay_lambda, sparsity_param, beta, patches), \n",
        "    method=\"L-BFGS-B\", \n",
        "    jac=True,\n",
        "    options={\"maxiter\":400,\"disp\":True});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Vwok9QzcYy"
      },
      "source": [
        "**STEP 5:** Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqy5uH8yzgQg"
      },
      "outputs": [],
      "source": [
        "W1 = res.x[0:hidden_size * visible_size].reshape(visible_size, hidden_size)\n",
        "display_network(W1.T, 5); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaFDDuAIXXoQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tufts-cs135-2022spring-ps3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}